# íŠœí† ë¦¬ì–¼: Upstage Solarë¥¼ í™œìš©í•œ Local-First Enterprise Doc-Informer ë§Œë“¤ê¸°

ì´ íŠœí† ë¦¬ì–¼ì€ ë¬¸ì„œë¥¼ ë¡œì»¬ì—ì„œ ì²˜ë¦¬í•˜ì—¬ ë¹„ìš©ì„ ì ˆê°í•˜ê³ , ê³ í’ˆì§ˆ ì¶”ë¡ ì„ ìœ„í•´ **Upstage Solar API**ë¥¼ í™œìš©í•˜ëŠ” RAG (Retrieval-Augmented Generation) ì• í”Œë¦¬ì¼€ì´ì…˜ì¸ **Local-First Enterprise Doc-Informer**ë¥¼ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ì‚¬ì „ ìš”êµ¬ ì‚¬í•­ (Prerequisites)

*   Python 3.10 ì´ìƒ ì„¤ì¹˜
*   Upstage API Key ([Upstage Console](https://console.upstage.ai/)ì—ì„œ ë°œê¸‰ ê°€ëŠ¥)
*   Python ë° ê°€ìƒ í™˜ê²½(Virtual Environments)ì— ëŒ€í•œ ê¸°ì´ˆ ì§€ì‹

## í”„ë¡œì íŠ¸ êµ¬ì¡° (Project Structure)

ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ í”„ë¡œì íŠ¸ë¥¼ êµ¬ì¶•í•  ê²ƒì…ë‹ˆë‹¤:

```text
local-doc-rag/
â”œâ”€â”€ data/                # PDF ë¬¸ì„œë¥¼ ì´ê³³ì— ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤.
â”œâ”€â”€ db/                  # ë²¡í„° ì„ë² ë”©ì„ ì €ì¥í•˜ëŠ” í´ë”ì…ë‹ˆë‹¤.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py        # PDF íŒŒì‹± ë° DB ì €ì¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
â”‚   â”œâ”€â”€ chain.py         # RAG íŒŒì´í”„ë¼ì¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.
â”‚   â””â”€â”€ eval.py          # ì„±ëŠ¥ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
â”œâ”€â”€ app.py               # Streamlit ëŒ€ì‹œë³´ë“œì…ë‹ˆë‹¤.
â””â”€â”€ requirements.txt
```

---

## Step 1: í™˜ê²½ ì„¤ì • (Environment Setup)

1.  **ê°€ìƒ í™˜ê²½ ìƒì„±:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Mac/Linux
    # venv\Scripts\activate  # Windows
    ```

2.  **ì˜ì¡´ì„± ì„¤ì¹˜:**
    `requirements.txt` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:
    ```text
    langchain-upstage
    langchain-community
    langchain-chroma
    langchain-text-splitters
    pymupdf
    unstructured
    ragas
    streamlit
    python-dotenv
    ```
    ì„¤ì¹˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤:
    ```bash
    pip install -r requirements.txt
    ```

3.  **API Key ì„¤ì •:**
    ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— `.env` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:
    ```env
    UPSTAGE_API_KEY=your_api_key_here
    ```

---

## Step 2: ê³ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘ (`src/ingest.py`)

PDFë¥¼ íŒŒì‹±í•˜ì—¬ ë¡œì»¬ ë²¡í„° DB(`ChromaDB`)ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤. ê°€ëŠ¥í•˜ë‹¤ë©´ `UpstageEmbeddings`ë¥¼ ì‚¬ìš©í•˜ê³ , ì™„ì „í•œ ë¡œì»¬ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ì„ ì›í•œë‹¤ë©´ `HuggingFaceBgeEmbeddings`ì™€ ê°™ì€ ë¡œì»¬ ëŒ€ì•ˆì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. *ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” í¸ì˜ë¥¼ ìœ„í•´ í‘œì¤€ Upstage í†µí•©ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì„ë² ë”© ëª¨ë¸ì€ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.*

```python
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_upstage import UpstageEmbeddings

load_dotenv()

def ingest_docs(pdf_path):
    # 1. PDF ë¡œë“œ
    print(f"Loading {pdf_path}...")
    loader = PyMuPDFLoader(pdf_path)
    docs = loader.load()

    # 2. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    splits = text_splitter.split_documents(docs)

    # 3. ë²¡í„° DBì— ì €ì¥
    print("Embedding and storing...")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
        persist_directory="./db"
    )
    print("Ingestion Complete!")

if __name__ == "__main__":
    # data/ í´ë”ì— PDFê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
    ingest_docs("./data/sample.pdf")
```

---

## Step 3: RAG íŒŒì´í”„ë¼ì¸ (`src/chain.py`)

ì´ì œ ê²€ìƒ‰(Retrieval) ë° ìƒì„±(Generation) ë¡œì§ì„ ìƒì„±í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë§¥ë½ì„ í™•ë³´í•˜ê¸° ìœ„í•´ **Max Marginal Relevance (MMR)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from langchain_chroma import Chroma
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def get_rag_chain():
    # 1. DB ì—°ê²°
    vectorstore = Chroma(
        persist_directory="./db",
        embedding_function=UpstageEmbeddings(model="solar-embedding-1-large")
    )
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 5, "fetch_k": 10}
    )

    # 2. í”„ë¡¬í”„íŠ¸ ì •ì˜
    template = """You are an assistant for analyzing enterprise documents.
    Answer the question based ONLY on the following context.
    If you cannot find the answer, say "I don't know based on this document."
    
    Context:
    {context}
    
    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 3. LLM ì •ì˜ (Solar)
    llm = ChatUpstage()

    # 4. ì²´ì¸ ì—°ê²°
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain
```

---

## Step 4: í‰ê°€ (`src/eval.py`)

ì´ ë‹¨ê³„ëŠ” "Production" ì§ë¬´ì—ì„œ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. `Ragas`ë¥¼ ì‚¬ìš©í•˜ì—¬ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevance
from datasets import Dataset
from src.chain import get_rag_chain

def run_evaluation():
    chain = get_rag_chain()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    questions = ["What is the main topic of the document?", "Explain the pricing model."]
    ground_truths = [["Topic A"], ["Pricing B"]] # ì¼ë¶€ ì§€í‘œ ì„ íƒ ì‹œ í•„ìš”
    
    answers = []
    contexts = []

    for q in questions:
        response = chain.invoke(q)
        answers.append(response)
        # ê°„ë‹¨í•œ ragas ì„¤ì •ì„ ìœ„í•´ ìˆ˜ë™ìœ¼ë¡œ contextë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ traceê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ëœ placeholderì…ë‹ˆë‹¤.
        contexts.append(["Context placeholder..."]) 

    data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }
    
    dataset = Dataset.from_dict(data)
    
    results = evaluate(
        dataset=dataset,
        metrics=[faithfulness, answer_relevance]
    )
    
    print(results)

if __name__ == "__main__":
    run_evaluation()
```

---

## Step 5: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (`app.py`)

ë§ˆì§€ë§‰ìœ¼ë¡œ, ë¬¸ì„œì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ Streamlit UIì…ë‹ˆë‹¤.

```python
import streamlit as st
from src.chain import get_rag_chain

st.title("ğŸ“„ Local-First Doc Informer")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ask about your document"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        chain = get_rag_chain()
        response = chain.invoke(prompt)
        st.markdown(response)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
```

## ì•± ì‹¤í–‰ (Running the App)

1.  `data/` í´ë”ì— PDF íŒŒì¼ì„ ë„£ìŠµë‹ˆë‹¤.
2.  ìˆ˜ì§‘(ingestion) ì‹¤í–‰: `python src/ingest.py`
3.  UI ì‹œì‘: `streamlit run app.py`

ê³ ì„±ëŠ¥ Local-First RAG ì•±ì„ ê²½í—˜í•´ë³´ì„¸ìš”!
